# -*- coding: utf-8 -*-
"""train skin diseases or lesion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19IcWdcq4dwxjN7gbChYp9wlv1t2vuyFn
"""

import tensorflow as tf

w = tf.constant(2.0)
x = tf.constant(3.0)
b = tf.constant(4.0)

y = tf.multiply(w,x)
y = tf.add(y,b)

print("Output:",y)

""" Import Libraries"""

!pip install --default-timeout=100 opencv-python

import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import sklearn
import numpy as np
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

"""DATA Preprocessing"""

## training image preprocessing

training_dataset = tf.keras.utils.image_dataset_from_directory(
    'train',
    labels="inferred",
    label_mode="categorical",
    class_names=None,
    color_mode="rgb",
    batch_size=19,
    image_size=(224,224),
    shuffle=True,
    seed=None,
    validation_split=None,
    subset=None,
    interpolation="bilinear",
    follow_links=False,
    crop_to_aspect_ratio=False,

)

import os
# Get the current working directory (i.e. the directory where your Python file is located)
current_dir = os.getcwd()

# Define the path to your dataset
path = os.path.join(current_dir, 'train')
path

class_names = sorted(os.listdir(path))
num_classes = len(class_names)

img_size = (192, 192, 3)
print('classes: ', class_names)

import cv2
import os

labels = []
images = []
print('images:\n')

for cl in class_names:
    print(cl, end=' -> ')
    cl_path = os.path.join(path, cl)  # Use os.path.join() to join the path and class name
    for img in os.listdir(cl_path):
        label = np.zeros(num_classes)
        label[class_names.index(cl)] = 1
        labels.append(label)
        image = np.asarray(cv2.resize(cv2.imread(os.path.join(cl_path, img), cv2.IMREAD_COLOR), img_size[0:2])[:, :, ::-1])
        images.append(image)

print('done')
labels = np.asarray(labels)
images = np.asarray(images)
print(f'\n\nlabels shape: {labels.shape}')
print(f'images shape: {images.shape}')

X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.15, random_state=42)

print(f'train images shape: {X_train.shape}\ntrain labels shape: {y_train.shape}\n\nvalidation images shape: {X_val.shape}\nvalidation labels shape: {y_val.shape}\n')

""" To avoid Overshotting
1. Choose small learning rate default 0.001 we are taking 0.0001
2. There may be chance of underfitting , so increase number of neurons
3. Add more convulation layer to extract more features from images there may be possibility that model unable to capture relevant feature or model is
 confusing due to lack of feature so feed with more feature
"""

model = tf.keras.Sequential()

# Inputs and rescaling
model.add(tf.keras.layers.Rescaling(scale=1. / 255, input_shape=(img_size)))

# Convolutional block 1
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

# Convolutional block 2
model.add(tf.keras.layers.Conv2D(128, (2, 2), activation='relu', padding='same'))
model.add(tf.keras.layers.Conv2D(128, (2, 2), activation='relu', padding='same'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

# Convolutional block 3
model.add(tf.keras.layers.Conv2D(256, (2, 2), activation='relu', padding='same'))
model.add(tf.keras.layers.Conv2D(256, (2, 2), activation='relu', padding='same'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))

# Convolutional block 4
model.add(tf.keras.layers.Conv2D(512, (2, 2), activation='relu', padding='same'))
model.add(tf.keras.layers.Conv2D(512, (2, 2), activation='relu', padding='same'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))
model.add(tf.keras.layers.Flatten())

# Dense block
model.add(tf.keras.layers.Dense(256, activation='relu'))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))

model.compile(optimizer='Adamax', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

from tensorflow.keras.callbacks import ModelCheckpoint

# creating ModelChecpoint callback
checkpoint_callback = ModelCheckpoint('model/model_{epoch:02d}.keras')

history = model.fit(images, labels, epochs=20, validation_data=(X_val, y_val), callbacks=[checkpoint_callback])

# Model Evaluation

# Model Evaluation on Training Set
train_loss,train_acc = model.evaluate(X_train, y_train)

## Model Evalution on Validation Set
train_loss,train_acc = model.evaluate(X_val,y_val)



# Save Model

model.save("trained_model.keras")

history.history

# Recording History in json
import json
with open("training_hist.json","w") as f:
    json.dump(history.history,f)

history.history['val_accuracy']

history.history['accuracy']

"""# Accuracy Visualization"""

epochs = [i for i in range(1, len(history.history['accuracy']) + 1)]
plt.plot(epochs, history.history['accuracy'], color='red', label='Training Accuracy')
plt.plot(epochs, history.history['val_accuracy'], color='blue', label='Validation Accuracy')
plt.xlabel("No. of epochs")
plt.ylabel("Accuracy Result")
plt.title("Visualization of Accuracy Result")
plt.show()



"""# Some other metrices for model evalution"""

class_names = test_set.class_names  # Use the original training dataset's class names
class_names

test_set = tf.keras.utils.image_dataset_from_directory(
    'test',
    labels="inferred",
    label_mode="categorical",
    class_names=None,
    color_mode="rgb",
    batch_size=20,
    image_size=((192, 192)),
    shuffle=False,
    seed=None,
    validation_split=None,
    subset=None,
    interpolation="bilinear",
    follow_links=False,
    crop_to_aspect_ratio=False,

)

model.summary()

for images, labels in test_set.take(1):
    print(images.shape)  # Should output: (batch_size, 192, 192, 3)

y_pred= model.predict(test_set)
y_pred,y_pred.shape

predicted_cat = tf.argmax(y_pred,axis=1)

predicted_cat

true_cat = tf.concat([y for x,y in test_set],axis=0)
true_cat

Y_true = tf.argmax(true_cat,axis=1)
Y_true

from sklearn.metrics import classification_report

print(classification_report(Y_true,predicted_cat,target_names=class_names))

from sklearn.metrics import confusion_matrix
cm= confusion_matrix(Y_true,predicted_cat)
cm

"""# Confusion Matrix Visualization"""

sns.heatmap(cm,annot=True)

